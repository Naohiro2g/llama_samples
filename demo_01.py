# https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF/tree/main
# モデルファイル Llama-3-ELYZA-JP-8B-q4_k_m.gguf をダウンロードして、modelsフォルダに保存する。
# GGUFは、CPU向け。GPUがある場合は、AWQを使うと良いらしいが未検証。

from llama_cpp import Llama

llm = Llama(
    model_path="models/Llama-3-ELYZA-JP-8B-q4_k_m.gguf",
    chat_format="llama-3",
    n_ctx=1024,
)

response = llm.create_chat_completion(
    messages=[
        {
            "role": "system",
            "content": "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。",
        },
        {
            "role": "user",
            "content": "以下の文章を要約してください。\
            # 文章\
            大規模言語モデル（だいきぼげんごモデル、英: large language model、LLM）は、多数のパラメータ（数千万から数十億）を持つ人工ニューラルネットワークで構成されるコンピュータ言語モデルで、膨大なラベルなしテキストを使用して自己教師あり学習または半教師あり学習（英語版）によって訓練が行われる。\
            LLMは2018年頃に登場し、さまざまなタスク（仕事）で優れた性能を発揮している。これにより、自然言語処理の研究の焦点は、特定のタスクに特化した教師ありモデルを訓練するという以前のパラダイムから転換した。大規模言語モデルの応用は目覚ましい成果を上げているが、大規模言語モデルの開発はまだ始まったばかりであり、多くの研究者が大規模言語モデルの改良に貢献している。\
            大規模言語モデルという用語の正式な定義はないが、大規模コーパスで事前訓練された、数百万から数十億以上のパラメータを持つディープラーニングモデルを指すことが多い。LLMは、特定のタスク（感情分析、固有表現抽出、数学的推論など）のために訓練されたものとは異なり、幅広いタスクに優れた汎用モデルである。LLMがタスクを実行する能力や対応可能な範囲は、ある意味では設計における画期的な進歩には依存せず、LLMに費やされた資源（データ、パラメータサイズ、計算力）の量の関数であるように見える。多数のパラメータを持ったニューラル言語モデルは、文の次の単語を予測するという単純なタスクで十分に訓練することで、人間の言葉の構文や意味の多くを捉えられることがわかった。さらに、大規模な言語モデルは、世の中に関するかなりの一般知識を示し、訓練中に大量の事実を「記憶」することができる。",
        },
    ],
    max_tokens=1024,
)

print(response["choices"][0]["message"]["content"])
